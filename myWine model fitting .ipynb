{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries and modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cqcplot import *\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold,KFold, cross_val_score\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set seed \n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### In this notebook I will load the cleaned data and \n",
    "1) fit a liner model \n",
    "2) fit other models (framed as both classification and regression) \n",
    "\n",
    "trying to predict variable 'quality' the problem can be framed as a regression task or a classification task. If it is taken as a classification task then this can be multi class or can be reduced to a binary classification, by say assuming all wines above 'good' are good and the rest have insufficient quality. The potential benefit of this approach is is that classes become a bit more balanced. This will be undertaken. \n",
    "\n",
    "From previous notebook Approx.80% of wines in dataset are nothing special (i.e. decent or worse), this should be a proxy for model evaluation. A simple strategy would be to always guess 'bad' wine, this would be 80% correct i.e 80% accuracy. This is the baseline to beat.  \n",
    "\n",
    "comment on evaluation metrics: problem has unbalanced classes so accuracy is not enough. In case of classification I will also use confusion matrix, precision and recall to evaluate performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " with open('./clean_wine_data.pickle', 'rb') as f:\n",
    "                data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6497, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape # have 15 features and a target column (quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unamed', 'fixed_acidity', 'volatile_acidity', 'citric_acid',\n",
       "       'residual_sugar', 'chlorides', 'free_sulfur_dioxide',\n",
       "       'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol',\n",
       "       'quality', 'technique', 'sweetness', 'wine_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make into binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isGoodWine(quality):\n",
    "    if quality >= 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wines = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wines['quality'] = wines['quality'].apply(isGoodWine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5218\n",
       "1    1279\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wines.quality.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 80/20 split with shuffling (default is true)\n",
    "y = wines['quality']\n",
    "X = wines.drop('quality', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   train_size = 0.8,\n",
    "                                                    random_state = seed, \n",
    "                                                    stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Define the scaler \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "# Scale the train set\n",
    "X_train = scaler.transform(X_train)\n",
    "# Scale the test set\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplest model : Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score, confusion_matrix,classification_report\n",
    "# instantiate a logistic regression model, and fit with X and y\n",
    "LR_model = LogisticRegression()\n",
    "LR_model = LR_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression TRAIN accuracy: 0.8190212373037857\n"
     ]
    }
   ],
   "source": [
    "# check the accuracy on the training set\n",
    "print('Logistic Regression TRAIN accuracy: {}' .format(LR_model.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST accuracy:0.8165024630541872\n",
      "ROC area:0.818303800915454\n"
     ]
    }
   ],
   "source": [
    "# evaluate \n",
    "predicted = LR_model.predict(X_test)\n",
    "# accuracy evaluation metrics\n",
    "print('TEST accuracy:{}' .format(accuracy_score(y_test, predicted)))\n",
    "# Area Under the Receiver Operating Characteristic Curve\n",
    "probs = LR_model.predict_proba(X_test)\n",
    "print ('ROC area:{}'.format(roc_auc_score(y_test, probs[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2567,   42],\n",
       "       [ 554,   85]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('confusion matrix:')\n",
    "confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is the same as when training and predicting on the same data.\n",
    "\n",
    "From confusion matrix acan see that a relatively large proportion of the minority class lables (ie. good wine) was missclasified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49411764705882355 minority class labels precision\n"
     ]
    }
   ],
   "source": [
    "print('{} minority class labels precision'.format(42/(85)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass clasisfictaion with  Trees \n",
    "use simple decision tree, a Gradient-Boosting classifier, and a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIT TREES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleTree = DecisionTreeClassifier(max_depth=5)\n",
    "simpleTree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbmTree = GradientBoostingClassifier(max_depth=5)\n",
    "gbmTree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfTree = RandomForestClassifier(max_depth=5)\n",
    "rfTree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate moedl preforemnce \n",
    "using Precision, Recall, F-Score, and Support measures for each classifier\n",
    "\n",
    "- Precision is a measure of a classifier’s exactness. The higher the precision, the more accurate the classifier.\n",
    "- Recall is a measure of a classifier’s completeness. The higher the recall, the more cases the classifier covers.\n",
    "- Looking at the support metric, can compare class-wise composition of the test population with the population as a whole. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simpleTreePerformance = precision_recall_fscore_support(y_test,simpleTree.predict(X_test))\n",
    "gbmTreePerformance = precision_recall_fscore_support(y_test,gbmTree.predict(X_test))\n",
    "rfTreePerformance = precision_recall_fscore_support(y_test,rfTree.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for each class in simple, gradient boosted, and random forest tree classifiers:\n",
      "\n",
      "Precision:  [ 0.88052681  0.5443038 ]\n",
      "Recall:  [ 0.89655172  0.50390625]\n",
      "Fscore:  [ 0.88846701  0.52332657]\n",
      "Support:  [1044  256] \n",
      "\n",
      "Precision:  [ 0.88251121  0.67567568]\n",
      "Recall:  [ 0.94252874  0.48828125]\n",
      "Fscore:  [ 0.91153312  0.56689342]\n",
      "Support:  [1044  256] \n",
      "\n",
      "Precision:  [ 0.82651391  0.56410256]\n",
      "Recall:  [ 0.96743295  0.171875  ]\n",
      "Fscore:  [ 0.89143866  0.26347305]\n",
      "Support:  [1044  256] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Metrics for each class in simple, gradient boosted, and random forest tree classifiers:'+'\\n')\n",
    "for treeMethod in [simpleTreePerformance,gbmTreePerformance,rfTreePerformance]:\n",
    "    print('Precision: ',treeMethod[0])\n",
    "    print('Recall: ',treeMethod[1])\n",
    "    print('Fscore: ',treeMethod[2])\n",
    "    print('Support: ',treeMethod[3],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns are the two clasess 'bad' and 'good' wine respectively. We are looking for values as close to 1 as possible. For classyfing the majority class all trees have performed better than the baseline 80% accuracy. \n",
    "Can see that the Gradient boosted tree achieved best results predicting the minority class with 67% precion and 49% recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking at support can see test population class composition very similarly distributed to population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class-composition: test set:0.24521072796934865, population:0.2451130701418168\n"
     ]
    }
   ],
   "source": [
    "print('class-composition: test set:{}, population:{}'.format((256/1044),(1279/5218)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for simple, gradient boosted, and random forest tree classifiers:\n",
      "Simple Tree:\n",
      " [[936 108]\n",
      " [127 129]] \n",
      "\n",
      "Gradient Boosted:\n",
      " [[984  60]\n",
      " [131 125]] \n",
      "\n",
      "Random Forest:\n",
      " [[1010   34]\n",
      " [ 212   44]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for simple, gradient boosted, and random forest tree classifiers:')\n",
    "print('Simple Tree:\\n',confusion_matrix(y_test, simpleTree.predict(X_test)),'\\n')\n",
    "print('Gradient Boosted:\\n',confusion_matrix(y_test, gbmTree.predict(X_test)),'\\n')\n",
    "print('Random Forest:\\n',confusion_matrix(y_test, rfTree.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32432432432432434"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('{} minority lables missclassifed by Gradient Boosted Tree'.format(60/(125+60))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that Gradient Boosted tree corectly classifies the largest number of instances (diagonal elments) it also corectly predicts the most cases in the minority class. It has lower numbers on the non-diagonal elements meaning less miss-classifctaion. Random Forest is teh worst and the simple tree sits in between. \n",
    "It should be noted that trees are not really much beter than Logistic Regression.\n",
    "\n",
    "THUS the Gradient Boosted tree should be put forward as the candidate tree model \n",
    "\n",
    "Can also explore relative feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances for GBM tree\n",
      "\n",
      "fixed acidity: 0.07157683953099729\n",
      "volatile acidity: 0.050763385291246396\n",
      "citric acid: 0.08913313361091399\n",
      "residual sugar: 0.061938656546516556\n",
      "chlorides: 0.07899814025647582\n",
      "free sulfur dioxide: 0.07906189742702219\n",
      "total sulfur dioxide: 0.06987227792111861\n",
      "density: 0.08109758979817112\n",
      "pH: 0.1093501202335202\n",
      "sulphates: 0.07448449381648868\n",
      "alcohol: 0.07897973792240626\n",
      "technique: 0.14601545340352806\n",
      "sweetness: 0.0006870647403854599\n",
      "wine_type: 3.527037747250596e-06\n"
     ]
    }
   ],
   "source": [
    "print('Feature Importances for GBM tree\\n')\n",
    "for importance,feature in zip(gbmTree.feature_importances_,['fixed acidity', 'volatile acidity', 'citric acid', \\\n",
    "'residual sugar','chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol','technique', 'sweetness', 'wine_type']):\n",
    "    print('{}: {}'.format(feature,importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can observe that pH, sulphates, fixed acidity had the most impact \n",
    "sweetness and wine_type had the least impact (possibly because they are derived from the other variables and thus dependent). Surprisingly technique has an impact, maybe due to the underlying generative process being related to the quality variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit simple NN: MLP \n",
    "1) make data into arrays\n",
    "2) make simple NN model\n",
    "3) standardise \n",
    "4) implemnet KFold CV and fir model\n",
    "5) evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = wines.quality.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wines.drop('quality',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = wines.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=4)\n",
    "skf.get_n_splits(X, Y)\n",
    "print(skf)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=15, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimators = []\n",
    "# Scale the data with `StandardScaler`\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "# run model as a classification problem \n",
    "estimators.append(('mlp', KerasClassifier(build_fn = baseline_model, epochs=20, batch_size=10, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "# apply KFold CV\n",
    "kfold = KFold(n_splits= 3, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv = kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.82 with std: 0.02\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAccuracy: %.2f with std: %.2f\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple NN learns reasonably well achieving approx 82% accuracy in 20 epochs averaged over 3 runs. This is slightly above the baseline strategy (achieving 80% accuracy) however it is not significantly better due to the standard deviation of the result. Tree models generally outperform the NN. \n",
    "However, If hyperparameters were tuned and the NN model trained for longer this would doubtlessly improve further and could potentially beat the trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclussion: \n",
    "\n",
    "Problem was framed as a binary classification and evaluated using Logistic Regression, different Tree models and a simple Neural Network. <br>\n",
    "The simple strategy of always guessing 'bad' wine would yield 80% accuracy. <br>\n",
    "Logistic Regression improved on this slightly by reaching an accuracy of approx 82%. However the precision on the minority class label prediction was approx. 49%. Further improvements could come from dropping insignificant features and transforming other variables. This was not undertaken due to time constraints <br>\n",
    "Trees performed slightly better, with Gradient boosted tree scoring the best among tree models. the GB_tree reached a precision of over 60% for the minority class and 88% for the majority class. Yielding it the best model thus far. <br>\n",
    "A simple NN was also attempted however it reached an accuracy of 82% with a standard deviation of 2% effectively yielding it as bad as the baseline. However, further tuning of the NN could markedly improve results, again this could not be undertaken due to time constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
